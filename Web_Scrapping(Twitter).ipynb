{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scrapping(Twitter).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wAexHzXPZaDq6Yktir1euNGg0fiQi6GK",
      "authorship_tag": "ABX9TyO9c4kByQ/AhofedYAuc6vh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KshitijMayank/Twitter_Scraping/blob/master/Web_Scrapping(Twitter).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvgn_Zb7DLM0",
        "colab_type": "text"
      },
      "source": [
        "**Personal Acces OAuth Details**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPOZ8BkOFhvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install twitter \n",
        "# pip install python-twitter\n",
        "# pip install tweepy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX2XS1eSaVeC",
        "colab_type": "text"
      },
      "source": [
        "Automating the code to fetch tweets from twitter based on the limits on API calls. This will fetch around ~15000 rows over the added parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOROgnrwDGIi",
        "colab_type": "text"
      },
      "source": [
        "**Setting up Connection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwppsMw1-9vD",
        "colab_type": "code",
        "outputId": "eff2d34d-e067-4d8f-e123-70753c3a4980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Importing tweepy\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "import tweepy\n",
        "import json\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "# import preprocessor as p\n",
        "# import os\n",
        "import time\n",
        "\n",
        "\n",
        "#getting our access Credentials...\n",
        "consumer_key='wGDrJqIQaGzEW4EN36ypSWCaY'\n",
        "consumer_secret='MOZ2jD2ryo4dJoDx6WsheFPVDMTsNwghpdc8q3D72tBg0FHpHp'\n",
        "access_token_key='1191970425095565313-EcKOnnwGrJ5WTY1TqpEd7vFq1g0hoi'\n",
        "access_token_secret='jF1LtDylkr1TrUAnaG6A6A1lXDGxxMQlUsOE9e1b6qQET'\n",
        "\n",
        "# Authenticating our python script\n",
        "# initialize api instance\n",
        "# Authenticate to Twitter\n",
        "auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
        "auth.set_access_token(access_token_key,access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "# test authentication\n",
        "try:\n",
        "    api.verify_credentials()\n",
        "    print(\"All good to go\")\n",
        "except:\n",
        "    print(\"Error during authentication\")\n",
        "\n",
        "# Automating the process to get tweets for the most popular election hashtags...\n",
        "\n",
        "\n",
        "def scraptweets(search_words, date_since, numTweets, numRuns):\n",
        "\n",
        "    ## Arguments:\n",
        "    # search_words -> define a string of keywords for this function to extract\n",
        "    # date_since -> define a date from which to start extracting the tweets \n",
        "    # numTweets -> number of tweets to extract per run\n",
        "    # numRun -> number of runs to perform in this program - API calls are limited to once every 15 mins, so each run will be 15 mins apart.\n",
        "    ##\n",
        "    \n",
        "    # Define a pandas dataframe to store the date:\n",
        "    db_tweets = pd.DataFrame(columns = ['username', 'acctdesc', 'location', 'following',\n",
        "                                        'followers', 'totaltweets', 'usercreatedts', 'tweetcreatedts',\n",
        "                                        'retweetcount', 'text', 'hashtags']\n",
        "                                )\n",
        "    # Define a for-loop to generate tweets at regular intervals\n",
        "    for i in range(0, numRuns):\n",
        "        # We will time how long it takes to scrape tweets for each run:\n",
        "        start_run = time.time()\n",
        "        \n",
        "        # Collect tweets using the Cursor object\n",
        "        # .Cursor() returns an object that you can iterate or loop over to access the data collected.\n",
        "        # Each item in the iterator has various attributes that you can access to get information about each tweet\n",
        "        tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since, tweet_mode='extended').items(numTweets)\n",
        "\n",
        "        # Store these tweets into a python list\n",
        "        tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "        # Obtain the following info (methods to call them out):\n",
        "            # user.screen_name - twitter handle\n",
        "            # user.description - description of account\n",
        "            # user.location - where is he tweeting from\n",
        "            # user.friends_count - no. of other users that user is following (following)\n",
        "            # user.followers_count - no. of other users who are following this user (followers)\n",
        "            # user.statuses_count - total tweets by user\n",
        "            # user.created_at - when the user account was created\n",
        "            # created_at - when the tweet was created\n",
        "            # retweet_count - no. of retweets\n",
        "            # (deprecated) user.favourites_count - probably total no. of tweets that is favourited by user\n",
        "            # retweeted_status.full_text - full text of the tweet\n",
        "            # tweet.entities['hashtags'] - hashtags in the tweet\n",
        "\n",
        "        # Begin scraping the tweets individually:\n",
        "        noTweets = 0\n",
        "\n",
        "        for tweet in tweet_list:\n",
        "\n",
        "            # Pull the values\n",
        "            username = tweet.user.screen_name\n",
        "            acctdesc = tweet.user.description\n",
        "            location = tweet.user.location\n",
        "            following = tweet.user.friends_count\n",
        "            followers = tweet.user.followers_count\n",
        "            totaltweets = tweet.user.statuses_count\n",
        "            usercreatedts = tweet.user.created_at\n",
        "            tweetcreatedts = tweet.created_at\n",
        "            retweetcount = tweet.retweet_count\n",
        "            hashtags = tweet.entities['hashtags']\n",
        "\n",
        "            try:\n",
        "                text = tweet.retweeted_status.full_text\n",
        "            except AttributeError:  # Not a Retweet\n",
        "                text = tweet.full_text\n",
        "\n",
        "            # Add the 11 variables to the empty list - ith_tweet:\n",
        "            ith_tweet = [username, acctdesc, location, following, followers, totaltweets,\n",
        "                         usercreatedts, tweetcreatedts, retweetcount, text, hashtags]\n",
        "\n",
        "            # Append to dataframe - db_tweets\n",
        "            db_tweets.loc[len(db_tweets)] = ith_tweet\n",
        "\n",
        "            # increase counter - noTweets  \n",
        "            noTweets += 1\n",
        "        \n",
        "        # Run ended:\n",
        "        end_run = time.time()\n",
        "        duration_run = round(end_run-start_run, 2)\n",
        "        \n",
        "        print('no. of tweets scraped for run {} is {}'.format(i, noTweets))\n",
        "        print('time take for {} run to complete is {}'.format(i, duration_run))\n",
        "        \n",
        "        time.sleep(900) #15 minute sleep time\n",
        "\n",
        "        \n",
        "    # Once all runs have completed, save them to a single csv file:    \n",
        "    # Obtain timestamp in a readable format:\n",
        "    from datetime import datetime\n",
        "    to_csv_timestamp = datetime.today().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "    # Define working path and filename\n",
        "    # path = os.getcwd()\n",
        "    filename = to_csv_timestamp + '_Elections(set7).csv'\n",
        "\n",
        "    # Store dataframe in csv with creation date timestamp\n",
        "    db_tweets.to_csv(filename, index = False)\n",
        "    \n",
        "    print('Scraping has completed!')\n",
        "\n",
        "\n",
        "#Our Lexicons for the program\n",
        "\n",
        "\n",
        "search_words = \"@BernieSanders OR @SenSanders OR #BernieSanders OR #Bernie2020 OR #BERNIE2020 OR #NotMeUs OR #MedicareForAll OR #FeelTheBern OR @JoeBiden OR  @TeamJoe OR #TeamJoe OR #Biden OR #Biden2020 OR #JoeBiden OR #TimesUpJoe OR #CNNTownHall OR #TimesUpBiden\"\n",
        "date_since = \"2020-15-02\"\n",
        "numTweets = 2500\n",
        "numRuns = 6\n",
        "# Call the function scraptweets\n",
        "scraptweets(search_words, date_since, numTweets, numRuns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All good to go\n",
            "no. of tweets scraped for run 0 is 0\n",
            "time take for 0 run to complete is 0.26\n",
            "no. of tweets scraped for run 1 is 0\n",
            "time take for 1 run to complete is 0.17\n",
            "no. of tweets scraped for run 2 is 0\n",
            "time take for 2 run to complete is 0.21\n",
            "no. of tweets scraped for run 3 is 0\n",
            "time take for 3 run to complete is 0.24\n",
            "no. of tweets scraped for run 4 is 0\n",
            "time take for 4 run to complete is 0.17\n",
            "no. of tweets scraped for run 5 is 0\n",
            "time take for 5 run to complete is 0.21\n",
            "Scraping has completed!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBjweJp41TZn",
        "colab_type": "text"
      },
      "source": [
        "### **Lexicons**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFJ2peRu0SOr",
        "colab_type": "code",
        "outputId": "2478afc3-9022-46ac-f90b-ad71b602ec74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\n",
        "# set5 ='#ForThePeople OR #2020Elections OR #hilory OR #Election2020 OR #Elections2020 OR #trump OR #makeamericagreatagain OR #republican OR #Democrats2020 OR #vote OR #trump OR  #voteblue OR #maga OR #impeachtrump OR #resist OR #donaldtrump OR #bernie OR #berniesanders2020'\n",
        "# set2='#america OR #joebiden OR #impeachment OR #democrat OR #electionday OR #berniesanders OR #democraticprimary OR #biden OR #pedovores OR #resist OR #mayorpete OR #andrewyang OR #walkaway OR #politicalmemes OR #impeachtrump OR #pedogate OR #liberals OR #gop OR #peteforamerica'\n",
        "# set1=\"#2020Elections OR #JoeBiden OR #Bernie2020 OR #Election2020 OR #Elections2020 OR #NotMeUs OR #makeamericagreatagain OR #electionday OR #berniesanders OR #joebiden OR #Democrats2020 OR #election OR #democrats OR #vote OR #trump OR @GeraldoRivera OR @realDonaldTrump OR @JoeBiden OR #CoronavirusPandemic OR @BernieSanders\"\n",
        "# set3=\"#liberal OR #progressive OR #politics OR #resistance OR #yanggang OR #trumpsucks OR #america OR #kag OR #usa OR #democrat OR #votebluenomatterwho OR #treason OR #petebuttigieg OR #qanon OR #votedemocrat OR #blackvotesmatter OR #getoutthevote\"\n",
        "# set4=\"#votebluetoendthisnightmare OR #bhfyp OR #america OR #joebiden OR #impeachment OR #democrat OR #electionday OR #berniesanders OR #democraticprimary OR #politics OR #pedovores OR #resist OR #mayorpete OR #andrewyang OR #walkaway OR #politicalmemes OR #impeachtrump OR #pedogate OR #liberals OR #gop OR #peteforamerica\"\n",
        "\n",
        "\n",
        "set7 =\"@BernieSanders OR @SenSanders OR #BernieSanders OR #Bernie2020 OR #BERNIE2020 OR #NotMeUs OR #MedicareForAll OR #FeelTheBern OR @JoeBiden OR  @TeamJoe OR #TeamJoe OR #Biden OR #Biden2020 OR #JoeBiden OR #TimesUpJoe OR #CNNTownHall OR #TimesUpBiden\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#america OR OR OR #joebiden OR OR OR #impeachment OR OR OR #democrat OR OR OR #electionday OR OR OR #berniesanders OR OR OR #democraticprimary OR OR OR #biden OR OR OR #pedovores OR OR OR #resist OR OR OR #mayorpete OR OR OR #andrewyang OR OR OR #walkaway OR OR OR #politicalmemes OR OR OR #impeachtrump OR OR OR #pedogate OR OR OR #liberals OR OR OR #gop OR OR OR #peteforamerica\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}